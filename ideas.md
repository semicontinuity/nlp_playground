* if word embeddings are sparse vectors, then, perhaps, training should adjust components of vector so,
that there is correlation between components of successive vectors?
* analogy: for SG Word2Vec, there are word vectors and context vectors: perhaps,
analogous to basal (signal) and apical (context) inputs of neurons?
* winner-take-all behavior of neurons and homeostasis: how to mimic?